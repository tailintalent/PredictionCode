{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Nov 14 15:34:10 2017\n",
    "dimensionality reduction and linear model.\n",
    "@author: monika\n",
    "\"\"\"\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.decomposition import PCA, FastICA, FactorAnalysis\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import explained_variance_score, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold, TimeSeriesSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import svm\n",
    "from sklearn.covariance import empirical_covariance\n",
    "import dataHandler as dh\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import newton \n",
    "from sklearn.cluster import bicluster\n",
    "from scipy.signal import welch, lombscargle, fftconvolve\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "# \n",
    "import makePlots as mp\n",
    "\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################    \n",
    "# \n",
    "# create trainings and test sets\n",
    "#\n",
    "############################################## \n",
    "def splitIntoSets(y, nBins=5, nSets=5, splitMethod='auto', verbose=0):\n",
    "    \"\"\"get balanced training sets for a dataset y.\"\"\"\n",
    "    # Determine limits of array\n",
    "    yLimits = np.array([np.min(y),np.max(y)])\n",
    "    #yLimits = np.percentile(y, [2.28, 97.72])#\n",
    "    if verbose >= 1:\n",
    "        print('Min and max y: ', yLimits)\n",
    "        print('Len(y): ', len(y))\n",
    "\n",
    "    # Calculate bin edges and number of events in each bin\n",
    "    nCounts,binEdges = np.histogram(y,nBins,yLimits)\n",
    "    if  verbose >= 2:\n",
    "        print('Bin edges: ', binEdges)\n",
    "        print('Counts: ', nCounts)\n",
    "\n",
    "    # Get minimum and maximum number of events in bins\n",
    "    nCountsLimits = np.array([np.min(nCounts),np.max(nCounts)])\n",
    "    if verbose >= 1:\n",
    "        print('Min and max counts: ', nCountsLimits)\n",
    "\n",
    "    # Determine bin index for each individual event\n",
    "    # Digitize is semi-open, i.e. slightly increase upper limit\n",
    "    binEdges[-1] += 1e-5\n",
    "    binEdges[0], binEdges[-1] = np.min(y), np.max(y)+1e-5\n",
    "    yBinIdx = np.digitize(y,binEdges) - 1\n",
    "    \n",
    "    # Get event indices for each bin\n",
    "    eventIndices = []\n",
    "    for binIdx in range(nBins):\n",
    "        #print np.where(yBinIdx == binIdx).shape\n",
    "        eventIndices.append(np.arange(len(y),dtype=int)[np.where(yBinIdx==binIdx)[0]])#[yBinIdx == binIdx])\n",
    "    eventIndices = np.asarray(eventIndices)\n",
    "\n",
    "    # Determine split method if auto is used\n",
    "    nPerBin = nCountsLimits[0]/nSets\n",
    "    if splitMethod == 'auto':\n",
    "        if nPerBin < 10:\n",
    "            splitMethod = 'redraw'\n",
    "        else:\n",
    "            splitMethod = 'unique'\n",
    "\n",
    "    # Get proper number of events per bin, depending on split method\n",
    "    if splitMethod == 'redraw':\n",
    "        nPerBin = nCountsLimits[1]/nSets                            # Maximum bin count divided by number of sets\n",
    "        if nPerBin > nCountsLimits[0]: nPerBin = nCountsLimits[0]   # But has to be at most the minimum bin count\n",
    "    else: nPerBin = nCountsLimits[0]/nSets\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Split method: ', splitMethod)\n",
    "        print('Events per bin per set: ', nPerBin)\n",
    "\n",
    "    # Create subsets\n",
    "    sets = [[] for i in range(nSets)]\n",
    "    for i in range(nBins):\n",
    "        _tEvtIdx = np.asarray(eventIndices[i][:])\n",
    "        for j in range(nSets):\n",
    "            np.random.shuffle(_tEvtIdx)\n",
    "            if len(_tEvtIdx) > nPerBin:\n",
    "                sets[j].append(np.copy(_tEvtIdx[:nPerBin]))\n",
    "            else:\n",
    "                sets[j].append(np.copy(_tEvtIdx[:]))\n",
    "            if splitMethod == 'unique':\n",
    "                _tEvtIdx = _tEvtIdx[nPerBin:]\n",
    "\n",
    "    # Convert into numpy arrays\n",
    "    for i in range(nSets):\n",
    "        sets[i] = np.asarray(sets[i]).reshape(nPerBin*nBins)\n",
    "    sets = np.asarray(sets)\n",
    "\n",
    "    # Prepare info dictionary with helpful data\n",
    "    info = {}\n",
    "    info['total-entries'] = int(sets.shape[0] * sets.shape[1])\n",
    "    info['unique-entries'] = len(list(set(np.ravel(sets))))\n",
    "    info['method'] = splitMethod\n",
    "    info['min-max-y'] = yLimits\n",
    "    info['min-max-cts'] = nCountsLimits\n",
    "    info['nbins'] = nBins\n",
    "\n",
    "    return sets, info\n",
    "    \n",
    "def createTrainingTestIndices(data, pars, label):\n",
    "    \"\"\"split time points into trainings and test set.\"\"\"\n",
    "    timeLen = data['Neurons']['Activity'].shape[1]\n",
    "    if pars['trainingType'] == 'start':\n",
    "        cutoff = int(pars['trainingCut']*timeLen)\n",
    "        testIndices = np.arange(timeLen)[:cutoff:]\n",
    "        trainingsIndices = np.arange(timeLen)[cutoff::pars['trainingSample']]\n",
    "    if pars['trainingType'] == 'simple':\n",
    "        cutoff = int(pars['trainingCut']*timeLen)\n",
    "        trainingsIndices = np.arange(timeLen)[:cutoff:pars['trainingSample']]\n",
    "        testIndices = np.arange(timeLen)[cutoff:]\n",
    "    elif pars['trainingType'] == 'random':\n",
    "        cutoff = int(pars['trainingCut']*timeLen)\n",
    "        tmpindices = np.arange(timeLen)\n",
    "        np.random.shuffle(tmpindices[::pars['trainingSample']])\n",
    "        trainingsIndices = np.sort(tmpindices[:cutoff])\n",
    "        testIndices = np.sort(tmpindices[cutoff:])\n",
    "    elif pars['trainingType'] == 'middle':\n",
    "        cutoff = int((pars['trainingCut'])*timeLen/2.)\n",
    "        testTime = int((1-pars['trainingCut'])*timeLen)\n",
    "        tmpIndices = np.arange(timeLen)\n",
    "#        if label =='Eigenworm3':\n",
    "#            cutoff = int((1-pars['trainingCut'])*timeLen/2.)\n",
    "#            loc = np.where(data['Behavior']['Ethogram']==2)[0]\n",
    "#            loc = loc[np.argmin(np.abs(loc-timeLen/2.))]\n",
    "#            testIndices = np.arange(np.max([0,loc-cutoff]), loc+cutoff)\n",
    "#        else:\n",
    "        # this makes a centered box\n",
    "        testIndices = tmpIndices[cutoff:-cutoff]\n",
    "        # this makes a box that starts in the center\n",
    "        #testIndices = tmpIndices[int(timeLen/2.):int(timeLen/2.)+testTime]\n",
    "        trainingsIndices = np.setdiff1d(tmpIndices, testIndices)[::pars['trainingSample']]\n",
    "    elif pars['trainingType'] == 'LR':\n",
    "        # crop out a testset first -- find an area that contains at least one turn\n",
    "        #center = np.where(np.abs(data['Behavior']['Eigenworm3'])>15)[0]\n",
    "        cutoff = int((pars['trainingCut'])*timeLen/2.)\n",
    "        \n",
    "        tmpIndices = np.arange(timeLen)\n",
    "        testIndices = tmpIndices[cutoff:-cutoff]\n",
    "        # create a trainingset by equalizing probabilities\n",
    "        trainingsIndices = np.setdiff1d(tmpIndices, testIndices)\n",
    "        # bin  to get probability distribution\n",
    "        nbin = 5\n",
    "        hist, bins = np.histogram(data['Behavior'][label], nbin)\n",
    "        # this is the amount of data that will be left in each bin after equalization\n",
    "        #N = np.sum(hist)/20.#hist[0]+hist[-1]\n",
    "        N = np.max(hist)/2#*nbin\n",
    "        print(bins, np.min(data['Behavior'][label]), np.max(data['Behavior'][label]))\n",
    "        # digitize data \n",
    "        dataProb = np.digitize(data['Behavior'][label], bins=bins[:-2], right=True)\n",
    "        # rescale such that we get desired trainingset length\n",
    "        trainingsIndices= []\n",
    "        \n",
    "        tmpTime = np.arange(0,timeLen)\n",
    "        \n",
    "        np.random.shuffle(tmpTime)\n",
    "        counter = np.zeros(hist.shape)\n",
    "        for index in tmpTime:\n",
    "                if index not in testIndices:\n",
    "                    # enrich for rare stuff\n",
    "                    n = dataProb[index]\n",
    "                    if counter[n] <= N:\n",
    "                        trainingsIndices.append(index)\n",
    "                        counter[n] +=1\n",
    "        print(len(trainingsIndices)/1.0/timeLen, len(testIndices)/1.0/timeLen)\n",
    "        plt.hist(data['Behavior'][label], normed=True,bins=nbin )\n",
    "        plt.hist(data['Behavior'][label][trainingsIndices], normed=True, alpha=0.5, bins=nbin)\n",
    "        plt.show()\n",
    "    return np.sort(trainingsIndices), np.sort(testIndices)\n",
    "    \n",
    "\n",
    "\n",
    "###############################################    \n",
    "# \n",
    "# PCA\n",
    "#\n",
    "##############################################\n",
    "\n",
    "def runPCANormal(data, pars, whichPC = 0, testset = None, deriv = False, useRaw=False):\n",
    "    \"\"\"run PCA on neural data and return nicely organized dictionary.\"\"\"\n",
    "    nComp = pars['nCompPCA']\n",
    "    pca = PCA(n_components = nComp)\n",
    "    if deriv:\n",
    "        Neuro = data['Neurons']['derivActivity']\n",
    "    if pars['useRank']:\n",
    "        Neuro = data['Neurons']['rankActivity']\n",
    "    if useRaw:\n",
    "        Neuro = data['Neurons']['Ratio']\n",
    "    else:\n",
    "        Neuro = np.copy(data['Neurons']['Activity'])\n",
    "    if testset is not None:\n",
    "        Yfull = np.copy(Neuro).T\n",
    "        Y = Neuro[:,testset].T\n",
    "    else:\n",
    "        Y= Neuro.T\n",
    "        Yfull = Y\n",
    "    # make sure data is centered\n",
    "    sclar= StandardScaler(copy=True, with_mean=True, with_std=False)\n",
    "    Y = sclar.fit_transform(Y)\n",
    "    # neuron activity is transposed such that result = nsamples*nfeatures.\n",
    "    comp = pca.fit_transform(Y).T\n",
    "    pcs = pca.components_.T\n",
    "    #print comp.shape, pcs.shape, 0/0\n",
    "    if deriv:\n",
    "        comp = np.cumsum(comp, axis=1)\n",
    "    \n",
    "    indices = np.argsort(pcs[:,whichPC])\n",
    "    \n",
    "    #print indices.shape\n",
    "    pcares = {}\n",
    "    pcares['nComp'] =  pars['nCompPCA']\n",
    "    pcares['expVariance'] =  pca.explained_variance_ratio_\n",
    "    pcares['eigenvalue'] =  pca.explained_variance_\n",
    "    pcares['neuronWeights'] =  pcs\n",
    "    pcares['neuronOrderPCA'] =  indices\n",
    "    pcares['pcaComponents'] =  comp\n",
    "    # reconstruct with nCompPCA\n",
    "    sclar2= StandardScaler(copy=True, with_mean=True, with_std=False)\n",
    "    Yfull = sclar2.fit_transform(Yfull)\n",
    "    compFull = pca.transform(Yfull).T\n",
    "    Yhat = np.dot(compFull.T[:,:nComp],pcs.T[:nComp,:])\n",
    "    Yhat += sclar2.mean_\n",
    "    if testset is not None:\n",
    "        pcares['testSet'] = testset\n",
    "    pcares['fullData'] = compFull\n",
    "    pcares['reducedData'] = Yhat.T\n",
    "    pcares['covariance'] = empirical_covariance(Yhat, assume_centered=False)\n",
    "#    plt.subplot(311)        \n",
    "#    plt.imshow(data['Neurons']['Activity'], aspect='auto')\n",
    "#    plt.subplot(312)\n",
    "#    plt.imshow(Yhat.T,  aspect='auto')\n",
    "#    plt.subplot(313)\n",
    "#    plt.imshow(pcares['covariance'])\n",
    "#    plt.show()\n",
    "    pcares['fullShuffle'], pcares['lagShuffle'] = runPCANoiseLevelEstimate(Y, pars)\n",
    "    return pcares\n",
    "###############################################    \n",
    "# \n",
    "# noise level estimate PCA\n",
    "#\n",
    "##############################################\n",
    "\n",
    "def runPCANoiseLevelEstimate(Y, pars):\n",
    "    \"\"\"run PCA on neural data and return nicely organized dictionary.\"\"\"\n",
    "    nComp = pars['nCompPCA']\n",
    "    pca = PCA(n_components = nComp)\n",
    "    # now we shuffle the data and calculate the variance explained\n",
    "    YS = np.array([np.random.permutation(n) for n in np.copy(Y).T]).T\n",
    "    # here we just randomly roll each timeseries by a certain amount i.e. lag or advance a neuron relative to others\n",
    "    Nmax = Y.shape[0]\n",
    "    YR = np.array([np.roll(n, np.random.randint(low=-Nmax, high=Nmax)) for n in Y.T]).T\n",
    "    \n",
    "    # make sure data is centered\n",
    "    sclar= StandardScaler(copy=True, with_mean=True, with_std=False)\n",
    "    YS = sclar.fit_transform(YS)\n",
    "    pca.fit(YS)\n",
    "    fullShuffle =  pca.explained_variance_\n",
    "    # make sure data is centered\n",
    "    sclar= StandardScaler(copy=True, with_mean=True, with_std=False)\n",
    "    YR = sclar.fit_transform(YR)\n",
    "    pca.fit(YR)\n",
    "    lagShuffle =  pca.explained_variance_\n",
    "#    plt.subplot(211)\n",
    "#    plt.imshow(YS.T, aspect='auto')\n",
    "#    plt.subplot(212)\n",
    "#    plt.imshow(YR.T, aspect='auto')\n",
    "#    plt.show()\n",
    "    return fullShuffle, lagShuffle\n",
    "    \n",
    "def runPCATimeWarp(data, pars):\n",
    "    \"\"\"run PCA on neural data and return nicely orgainzed dictionary.\"\"\"\n",
    "    nComp = pars['nCompPCA']\n",
    "    pca = PCA(n_components = nComp)\n",
    "    neurons = timewarp(data)\n",
    "    pcs = pca.fit_transform(neurons.T)\n",
    "    \n",
    "    pcs = pca.transform(np.copy(data['Neurons']['Activity']).T)\n",
    "    \n",
    "    # order neurons by weight in first component\n",
    "    indices = np.arange(len( data['Neurons']['Activity']))\n",
    "    indices = np.argsort(pca.components_[0])\n",
    "    \n",
    "    pcares = {}\n",
    "    pcares['nComp'] =  pars['nCompPCA']\n",
    "    pcares['expVariance'] =  pca.explained_variance_ratio_\n",
    "    pcares['neuronWeights'] =  pca.components_.T\n",
    "    pcares['neuronOrderPCA'] =  indices\n",
    "    pcares['pcaComponents'] =  pcs.T\n",
    "    \n",
    "    return pcares\n",
    "\n",
    "def timewarp(data):\n",
    "    \"\"\"creates a subsampled neuron signal for PCA that has equally represented behaviors.\"\"\"\n",
    "    # find out how much fwd and backward etc we have:\n",
    "    #labelDict = {-1:'Reverse',0:'Pause',1:'Forward',2:'Turn'}\n",
    "    neur = np.copy(data['Neurons']['Activity'])\n",
    "    # find how often each behavior occurs\n",
    "    indices = []\n",
    "    for bindex, behavior in enumerate([-1,1,2]):\n",
    "        # find behavior indices\n",
    "        indices.append(np.where(data['Behavior']['Ethogram']==behavior)[0])\n",
    "    # number of timestamps in each behavior\n",
    "    lens = np.array([len(x) for x in indices])\n",
    "    # timesamples in the smallest behavior\n",
    "    minval = np.min(lens[np.nonzero(lens)])\n",
    "    #subsample neural data to the minimal, non-zero size\n",
    "    neurArr = []\n",
    "    for i in range(3):\n",
    "        \n",
    "        if lens[i] !=0:\n",
    "#            # this subsamples\n",
    "#            useOnly = np.arange(0,lens[i], np.int(lens[i]/minval))[:minval]\n",
    "#            tmp = neur[:,indices[i][useOnly]]\n",
    "#            neurArr.append(tmp)\n",
    "            # this averages\n",
    "            tmp = neur[:,indices[i]]\n",
    "            end =  minval * int(lens[i]/minval)\n",
    "            neurArr.append(np.mean(tmp[:,:end].reshape(tmp.shape[0],-1, minval), 1))\n",
    "            \n",
    "    neurData = np.concatenate(neurArr, axis=1)\n",
    "    return neurData\n",
    "\n",
    "def rankCorrPCA(results):\n",
    "    \"\"\"correlate the first and second half PCA weights by rank.\"\"\"\n",
    "    tmpdata = np.zeros((3,3))\n",
    "    for pc1 in range(3):\n",
    "        for pc2 in range(3):\n",
    "            # rank correlation\n",
    "            #rankHalf1 = np.argsort(results['PCAHalf1']['neuronWeights'][:,pc1])\n",
    "            #rankHalf2 = np.argsort(results['PCAHalf2']['neuronWeights'][:,pc2])\n",
    "            #tmpdata[pc1, pc2] = np.corrcoef(rankHalf1, rankHalf2)[0,1]\n",
    "            #dot product instead\n",
    "            v1 = results['PCAHalf1']['neuronWeights'][:,pc1]\n",
    "            v2 = results['PCAHalf2']['neuronWeights'][:,pc2]\n",
    "            tmpdata[pc1, pc2] = np.dot(v1, v2)/np.linalg.norm(v1)/np.linalg.norm(v2)\n",
    "    return tmpdata\n",
    "    \n",
    "###############################################    \n",
    "# \n",
    "# correlate neurons and behavior\n",
    "#\n",
    "##############################################\n",
    "def behaviorCorrelations(data, behaviors, subset = None):\n",
    "    \"\"\"simple r2 scores of behavior and neural activity.\"\"\"\n",
    "    Y = np.copy(data['Neurons']['Activity'])\n",
    "    print(Y.shape)\n",
    "    nNeur = Y.shape[0]\n",
    "    results = {}\n",
    "    \n",
    "    for bindex, beh in enumerate(behaviors):\n",
    "        r2s = []\n",
    "        x = data['Behavior'][beh]\n",
    "        if subset is not None:\n",
    "            print(max(subset), Y.shape)\n",
    "            Y = Y[:,subset]\n",
    "            x = x[subset]\n",
    "        x = (x-np.mean(x))/np.std(x)\n",
    "        for n in range(nNeur):\n",
    "            r2s.append(np.corrcoef(x, Y[n])[0,1]**2)\n",
    "        results[beh] = np.array(r2s)\n",
    "    return results\n",
    "    \n",
    "    \n",
    "###############################################    \n",
    "# \n",
    "# correlate neurons and PCA axes\n",
    "#\n",
    "##############################################\n",
    "def PCACorrelations(data,results, behaviors, flag = 'PCA', subset = None):\n",
    "    \"\"\"simple r2 scores of behavior and neural activity.\"\"\"\n",
    "    Y = results[flag]['pcaComponents'][:3,]\n",
    "    \n",
    "    nNeur = Y.shape[0]\n",
    "    results = {}\n",
    "    \n",
    "    for bindex, beh in enumerate(behaviors):\n",
    "        r2s = []\n",
    "        x = data['Behavior'][beh]\n",
    "        if subset is not None:\n",
    "            print(max(subset), Y.shape)\n",
    "            Y = Y[:,subset]\n",
    "            x = x[subset]\n",
    "        x = (x-np.mean(x))/np.std(x)\n",
    "        for n in range(nNeur):\n",
    "            r2s.append(np.corrcoef(x, Y[n])[0,1]**2)\n",
    "        results[beh] = np.array(r2s)\n",
    "    return results\n",
    "###############################################    \n",
    "# \n",
    "# estimate signal/ periodogram\n",
    "#\n",
    "##############################################\n",
    "def runPeriodogram(data, pars, testset = None):\n",
    "    \"\"\"run a welch periodogram to estimate the PSD of neural activity.\"\"\"\n",
    "    Neuro = np.copy(data['Neurons']['Activity'])\n",
    "    time = data['Neurons']['Time']\n",
    "    B = np.array(data['Behavior']['Ethogram'], dtype=float)\n",
    "    \n",
    "    if testset is not None:\n",
    "        Neuro = np.array(Neuro)[:,testset]\n",
    "        time = time[testset]\n",
    "        B = B[testset]\n",
    "    Neuro -=np.mean(Neuro, axis=0)\n",
    "    Neuro /=np.std(Neuro, axis=0)\n",
    "    B -=np.mean(B)\n",
    "    B/= np.std(B)\n",
    "    autocorr = np.array([fftconvolve(y, y[::-1], mode='full')/len(y) for y in Neuro])\n",
    "    periods = np.arange(-len(y), len(y)-1)/6. # in seconds\n",
    "    # behavior\n",
    "    autocorrB = np.array(fftconvolve(B, B[::-1], mode='full')/len(B))\n",
    "    # get only relevant subset\n",
    "    Indices = np.rint(np.interp(pars['periods'], periods, np.arange(len(periods)))).astype(int)\n",
    "#    plt.plot(np.mean(autocorr[:,Indices], axis=0))\n",
    "#    plt.show()\n",
    "#    plt.plot(periods[Indices],np.mean(autocorr, axis = 0)[Indices])\n",
    "#    plt.plot(periods[Indices],autocorrB[Indices])\n",
    "\n",
    "#    plt.show()\n",
    "    results = {}\n",
    "    results['BehaviorACorr'] = autocorrB[Indices]\n",
    "    results['NeuronACorr'] = autocorr[:,Indices]\n",
    "    results['Periods'] = periods[Indices]\n",
    "    return results\n",
    "###############################################    \n",
    "# \n",
    "# hierarchical clustering\n",
    "#\n",
    "##############################################    \n",
    "\n",
    "def runHierarchicalClustering(data, pars, subset):\n",
    "    \"\"\"cluster neural data.\"\"\"\n",
    "    \n",
    "    X = np.copy(data['Neurons']['Ratio']) # transpose to conform to nsamples*nfeatures\n",
    "    if subset is not None:\n",
    "        X = X[subset]\n",
    "    # pairwise correlations\n",
    "    C = np.ma.corrcoef(X)\n",
    "    # find linkage\n",
    "    Z = linkage(X, 'ward')\n",
    "#    # assign clusters\n",
    "    max_d = 1.5\n",
    "#    clusters = fcluster(Z, max_d, criterion='distance')\n",
    "    # assign clusters\n",
    "    k=3\n",
    "    clusters = fcluster(Z, k, criterion='maxclust')\n",
    "    traces = []\n",
    "    \n",
    "    for index in np.unique(clusters):\n",
    "        traces.append(X[np.where(clusters==index)])\n",
    "    # store results\n",
    "    clustres = {}\n",
    "    clustres['linkage'] = Z\n",
    "    clustres['clusters'] = traces\n",
    "    clustres['leafs'] = clusters\n",
    "    clustres['nclusters'] = len(np.unique(clusters))\n",
    "    clustres['dmax'] = max_d\n",
    "    clustres['threshold'] = Z[-(k-1),2]  \n",
    "\n",
    "    return clustres\n",
    "###############################################    \n",
    "# \n",
    "# predict discrete behaviors\n",
    "#\n",
    "##############################################\n",
    "    \n",
    "def discreteBehaviorPrediction(data, pars, splits):\n",
    "    \"\"\"use a svm to predict discrete behaviors from the data.\"\"\"\n",
    "    # modify data to be like scikit likes it\n",
    "    if pars['useRank']:\n",
    "            X = data['Neurons']['rankActivity'].T\n",
    "    else:\n",
    "        X = np.copy(data['Neurons']['Activity']).T # transpose to conform to nsamples*nfeatures\n",
    "    # use ethogram for behavior\n",
    "    Y = data['Behavior']['Ethogram']\n",
    "    label = 'AngleVelocity'\n",
    "    trainingsInd, testInd = splits[label]['Train'], splits[label]['Test']\n",
    "    # create a linear SVC\n",
    "    lin_clf = svm.LinearSVC(penalty='l1',dual=False, class_weight='balanced', C=10)\n",
    "    lin_clf.fit(X[trainingsInd], np.array(list(Y))[trainingsInd]) \n",
    "    Ypred = lin_clf.predict(X[testInd])\n",
    "    \n",
    "    print(classification_report(np.array(list(Y))[testInd], Ypred))\n",
    "    pcs = lin_clf.coef_\n",
    "    indices = np.argsort(pcs[0])\n",
    "    \n",
    "    comp = np.zeros((4, len(X)))\n",
    "    # show temporal components\n",
    "    for wi, weights in enumerate(pcs):\n",
    "        comp[wi] = np.dot(X, weights)\n",
    "    #print f1_score(np.array(list(Y))[testInd], Ypred, average='micro')\n",
    "    recision, recall, fscore, support = precision_recall_fscore_support(np.array(list(Y))[testInd], Ypred, labels=[-1,0,1,2])\n",
    "    print(fscore)\n",
    "    pcares = {}\n",
    "    pcares['nComp'] =  4\n",
    "    pcares['expVariance'] =  fscore\n",
    "    pcares['neuronWeights'] =  pcs.T\n",
    "    pcares['neuronOrderPCA'] =  indices\n",
    "    pcares['pcaComponents'] =  comp\n",
    "    T = testInd\n",
    "    ax1 = plt.subplot(211)\n",
    "    mp.plotEthogram(ax1, T, np.array(list(Y))[testInd], alpha = 0.5, yValMax=1, yValMin=0, legend=0)\n",
    "    ax1 = plt.subplot(212)\n",
    "    mp.plotEthogram(ax1, T, Ypred, alpha = 0.5, yValMax=1, yValMin=0, legend=0)\n",
    "    return pcares\n",
    "###############################################    \n",
    "# \n",
    "# projection using behavior triggered averages\n",
    "#\n",
    "##############################################  \n",
    "def runBehaviorTriggeredAverage(data, pars):\n",
    "    \"\"\"use averaging of behaviors to get neural activity corresponding.\"\"\"\n",
    "    # modify data to be like scikit likes it\n",
    "    if pars['useRank']:\n",
    "            Y = data['Neurons']['rankActivity'].T\n",
    "    if pars['useClust']:\n",
    "        clustres = runHierarchicalClustering(data, pars)\n",
    "        Y = clustres['Activity'].T\n",
    "    else:\n",
    "        Y = np.copy(data['Neurons']['Activity']).T # transpose to conform to nsamples*nfeatures\n",
    "    # use ethogram for behavior\n",
    "    X = data['Behavior']['Ethogram']\n",
    "    \n",
    "    orderFwd = np.argsort(np.std(Y, axis=0))\n",
    "    pcs = np.zeros((4, Y.shape[1]))\n",
    "    for index, bi in enumerate([1,-1, 2, 0]):\n",
    "        indices = np.where(X==bi)[0]\n",
    "        Ynew = Y[indices]\n",
    "        pcs[index] = np.mean(Ynew, axis=0)\n",
    "    # project data onto components\n",
    "    comp = np.zeros((4, len(Y)))\n",
    "    for wi, weights in enumerate(pcs):\n",
    "        comp[wi] = np.dot(Y, weights)\n",
    "        # backcalculate explained variance\n",
    "        \n",
    "    # calculate explained variance\n",
    "    #explained_variance_score()\n",
    "    # write to a results dictionary\n",
    "    pcares = {}\n",
    "    pcares['nComp'] =  4\n",
    "    pcares['expVariance'] =  np.arange(4)\n",
    "    pcares['neuronWeights'] =  pcs.T\n",
    "    pcares['neuronOrderPCA'] =  orderFwd\n",
    "    pcares['pcaComponents'] =  comp\n",
    "    return pcares\n",
    "\n",
    "###############################################    \n",
    "# \n",
    "# Linear Model\n",
    "#\n",
    "##############################################    \n",
    "\n",
    "def runLinearModel(data, results, pars, splits, plot = False, behaviors = ['AngleVelocity', 'Eigenworm3'], fitmethod = 'LASSO', subset = None):\n",
    "    \"\"\"run a linear model to fit behavior and neural activity with a linear model.\"\"\"\n",
    "    linData = {}\n",
    "    for label in behaviors:\n",
    "        Y = data['Behavior'][label]\n",
    "        trainingsInd, testInd = splits[label]['Train'], splits[label]['Test']\n",
    "    \n",
    "        if pars['useRank']:\n",
    "            X = data['Neurons']['rankActivity'].T\n",
    "        if pars['useClust']:\n",
    "            clustres = runHierarchicalClustering(data, pars)\n",
    "            X = clustres['Activity'].T\n",
    "        else:\n",
    "            X = np.copy(data['Neurons']['Activity']).T # transpose to conform to nsamples*nfeatures\n",
    "        if subset is not None:\n",
    "            # only a few neurons\n",
    "            if len(subset[label])<1:\n",
    "                print('no weights found.proceeding with all neurons')\n",
    "            else:\n",
    "                X = X[:,subset[label]]\n",
    "#        cv = 10\n",
    "        reg = linear_model.LinearRegression()\n",
    "        # setting alpha to zero makes this a linear model without regularization\n",
    "        reg = linear_model.Lasso(alpha = results[fitmethod][label]['alpha'])\n",
    "        reg.fit(X[trainingsInd], np.array(list(Y))[trainingsInd])\n",
    "#        alphas = reg.alphas_\n",
    "#        ymean = np.mean(reg.mse_path_, axis =1)\n",
    "#        yerr = np.std(reg.mse_path_, axis =1)/np.sqrt(cv)\n",
    "#        alphaNew = alphas[np.argmin(ymean)]\n",
    "#        # calculate standard deviation rule\n",
    "#        alphaNew = stdevRule(x = alphas, y= ymean, std= yerr)\n",
    "#        reg = linear_model.Lasso(alpha=alphaNew)\n",
    "#        reg.fit(X[trainingsInd], np.array(list(Y))[trainingsInd])\n",
    "        \n",
    "        if plot:\n",
    "            plt.subplot(221)\n",
    "            plt.title('Trainingsset')\n",
    "            plt.plot(np.array(list(Y))[trainingsInd], 'r')\n",
    "            plt.plot(reg.predict(X[trainingsInd]), 'k', alpha=0.7)\n",
    "            plt.subplot(222)\n",
    "            plt.title('Testset')\n",
    "            plt.plot(np.array(list(Y))[testInd], 'r')\n",
    "            plt.plot(reg.predict(X[testInd]), 'k', alpha=0.7)\n",
    "            ax1 = plt.subplot(223)\n",
    "            plt.title('Non-zero weighths: {}'.format(len(reg.coef_[reg.coef_!=0])))\n",
    "            ax1.scatter(np.array(list(Y))[testInd], reg.predict(X[testInd]), alpha=0.7, s=0.2)\n",
    "#            hist, bins = np.histogram(reg.coef_, bins = 30, density = True)\n",
    "#            ax1.fill_between(bins[:-1],np.zeros(len(hist)), hist, step='post', color='r')\n",
    "#            ax1.set_xlabel(r'weights')\n",
    "#            ax1.set_ylabel('PDF(weights)')\n",
    "            plt.subplot(224)\n",
    "#            \n",
    "#            #plt.plot(reg.alphas_, reg.mse_path_, 'k', alpha = 0.3)\n",
    "#            plt.plot(alphas, ymean, 'k')\n",
    "#            plt.errorbar(alphas, ymean, yerr=yerr, capsize=1)\n",
    "#            plt.axvline(alphas[np.argmin(ymean)],label='minimal error')\n",
    "#            plt.axvline(alphaNew,label='stdev rule')\n",
    "#            plt.xscale('log')\n",
    "#            #plt.fill_between(reg.alphas_,y1=ymean-yerr, y2= ymean+yerr, alpha=0.5)\n",
    "#            #plt.errorbar(,color= 'k')\n",
    "            plt.tight_layout()            \n",
    "            plt.show()\n",
    "        weights = reg.coef_\n",
    "        # score model\n",
    "        if len(weights)>0:\n",
    "            scorepred = reg.score(X[testInd], np.array(list(Y))[testInd])#, sample_weight=np.power(np.array(list(Y))[testInd], 2))\n",
    "            score = reg.score(X[trainingsInd], np.array(list(Y))[trainingsInd])\n",
    "        else:\n",
    "            scorepred = np.nan\n",
    "            score = reg.score(X[trainingsInd], np.array(list(Y))[trainingsInd])\n",
    "        linData[label] = {}\n",
    "        linData[label]['weights'] =  weights\n",
    "        linData[label]['fullweights'] =  weights\n",
    "        if subset[label] is not None and len(subset[label])>0:\n",
    "            fullweights = np.zeros(data['Neurons']['Activity'].shape[0])\n",
    "            fullweights[subset[label]] = weights\n",
    "            linData[label]['fullweights'] =  fullweights\n",
    "        linData[label]['intercepts'] = reg.intercept_\n",
    "#        linData[label]['alpha'] = alphaNew#reg.alpha_\n",
    "        linData[label]['score'] = score\n",
    "        linData[label]['scorepredicted'] = scorepred\n",
    "        linData[label]['noNeurons'] = len(reg.coef_[np.abs(reg.coef_)>0])\n",
    "        linData[label]['output'] = reg.predict(X) # full output training and test\n",
    "        print('r2', scorepred)\n",
    "    return linData\n",
    "\n",
    "\n",
    "###############################################    \n",
    "# \n",
    "# LASSO\n",
    "#\n",
    "##############################################   \n",
    "def runLassoLars(data, pars, splits, plot = False, behaviors = ['AngleVelocity', 'Eigenworm3'], lag = None):\n",
    "    \"\"\"run LASSO to fit behavior and neural activity with a linear model.\"\"\"\n",
    "    linData = {}\n",
    "    for label in behaviors:\n",
    "        Y = np.reshape(np.copy(data['Behavior'][label]),(-1, 1))\n",
    "        trainingsInd, testInd = splits[label]['Train'], splits[label]['Test']\n",
    "    \n",
    "        if pars['useRank']:\n",
    "            X = data['Neurons']['rankActivity'].T\n",
    "        elif pars['useClust']:\n",
    "            clustres = runHierarchicalClustering(data, pars)\n",
    "            X = clustres['Activity'].T\n",
    "        else:\n",
    "            X = np.copy(data['Neurons']['Activity'].T) # transpose to conform to nsamples*nfeatures\n",
    "        # implement time lagging -- forward and reverse\n",
    "        \n",
    "        if lag is not None:\n",
    "            # move neural activity by lag (lag has units of volume)\n",
    "            # positive lag = behavior trails after neural activity, uses values from the past\n",
    "            # negative lag = behavior precedes neural activity\n",
    "            X = np.roll(X, shift = lag, axis = 0)\n",
    "        # prep data by scalig\n",
    "        # fit scale model\n",
    "        scale = True\n",
    "        if scale:\n",
    "            scalerX = preprocessing.StandardScaler().fit(X[trainingsInd])  \n",
    "            scalerY = preprocessing.StandardScaler().fit(np.array(list(Y))[trainingsInd])  \n",
    "            #scale data\n",
    "            X = scalerX.transform(X)\n",
    "            Y = scalerY.transform(Y)\n",
    "        Xtrain, Xtest = X[trainingsInd],X[testInd]\n",
    "        Ytrain, Ytest = np.array(list(Y))[trainingsInd],np.array(list(Y))[testInd]\n",
    "        # fit lasso and validate\n",
    "        #a = np.logspace(-2,2,100)\n",
    "        cv = 10\n",
    "        # unbalanced sets\n",
    "#        fold = KFold(cv, shuffle=True) \n",
    "#        # balanced sets\n",
    "        if label =='Eigenworm3':\n",
    "            a = np.logspace(-3,-1,100)\n",
    "        else:\n",
    "            a = np.logspace(-3,0,100)\n",
    "        if label =='Eigenworm3':\n",
    "            fold = balancedFolds(np.array(list(Y))[trainingsInd], nSets=cv)\n",
    "##        else:\n",
    "#        fold = balancedFolds(np.array(list(Y))[trainingsInd], nSets=cv)\n",
    "        fold = 5\n",
    "        fold = TimeSeriesSplit(n_splits=5, max_train_size=None)\n",
    "        reg = linear_model.LassoLarsIC(criterion = 'bic',  verbose=0, \\\n",
    "         max_iter=5000)#, eps=1e-2)#, normalize=False)\n",
    "        \n",
    "        reg.fit(Xtrain, Ytrain)\n",
    "        alphas= reg.alphas_\n",
    "        ymean = reg.criterion_\n",
    "        yerr = None\n",
    "        alphaNew = reg.alpha_\n",
    "        ####for crossval\n",
    "        #alphas = reg.cv_alphas_\n",
    "        #ymean = np.mean(reg.mse_path_, axis =1)\n",
    "        #yerr = np.std(reg.mse_path_, axis =1)/np.sqrt(cv)\n",
    "        #alphaNew = alphas[np.argmin(ymean)]\n",
    "#        # calculate standard deviation rule\n",
    "#        alphaNew = stdevRule(x = alphas, y= ymean, std= yerr)\n",
    "#        reg = linear_model.Lasso(alpha=alphaNew)\n",
    "#        reg.fit(X[trainingsInd], np.array(list(Y))[trainingsInd])\n",
    "        \n",
    "        if plot:\n",
    "            plt.subplot(221)\n",
    "            plt.title('Trainingsset')\n",
    "            plt.plot(Ytrain, 'r')\n",
    "            plt.plot(reg.predict(Xtrain), 'k', alpha=0.7)\n",
    "            plt.subplot(222)\n",
    "            plt.title('Testset')\n",
    "            plt.plot(np.array(list(Y))[testInd], 'r')\n",
    "            plt.plot(reg.predict(Xtest), 'k', alpha=0.7)\n",
    "            ax1 = plt.subplot(223)\n",
    "            plt.title('Non-zero weighths: {}'.format(len(reg.coef_[reg.coef_!=0])))\n",
    "            ax1.scatter(Ytest, reg.predict(Xtest), alpha=0.7, s=0.2)\n",
    "#            hist, bins = np.histogram(reg.coef_, bins = 30, density = True)\n",
    "#            ax1.fill_between(bins[:-1],np.zeros(len(hist)), hist, step='post', color='r')\n",
    "#            ax1.set_xlabel(r'weights')\n",
    "#            ax1.set_ylabel('PDF(weights)')\n",
    "            plt.subplot(224)\n",
    "            \n",
    "            #plt.plot(alphas, reg.mse_path_, 'k', alpha = 0.3)\n",
    "            plt.plot(alphas, ymean, 'k')\n",
    "            plt.errorbar(alphas, ymean, yerr=yerr, capsize=1)\n",
    "            plt.axvline(alphas[np.argmin(ymean)],label='minimal error')\n",
    "            #plt.axvline(alphaNew,label='stdev rule')\n",
    "            plt.xscale('log')\n",
    "            #plt.fill_between(reg.alphas_,y1=ymean-yerr, y2= ymean+yerr, alpha=0.5)\n",
    "            #plt.errorbar(,color= 'k')\n",
    "            plt.tight_layout()            \n",
    "            plt.show(block=True)\n",
    "        weights = reg.coef_\n",
    "        # score model\n",
    "        if len(weights)>0:\n",
    "            # normalize testset with scaler values\n",
    "            scorepred = reg.score(Xtest, Ytest)#, sample_weight=np.power(np.array(list(Y))[testInd], 2))\n",
    "            score = reg.score(Xtrain, Ytrain)\n",
    "        else:\n",
    "            scorepred = np.nan\n",
    "            score = reg.score(Xtrain, Ytrain)\n",
    "        linData[label] = {}\n",
    "        linData[label]['weights'] =  weights\n",
    "        linData[label]['intercepts'] = reg.intercept_\n",
    "        linData[label]['alpha'] = alphaNew#reg.alpha_\n",
    "        linData[label]['score'] = score\n",
    "        linData[label]['scorepredicted'] = scorepred\n",
    "        linData[label]['noNeurons'] = len(reg.coef_[np.abs(reg.coef_)>0])\n",
    "        if scale:\n",
    "            linData[label]['output'] = scalerY.inverse_transform(reg.predict(X)) # full output training and test\n",
    "        else:\n",
    "            linData[label]['output'] = reg.predict(X)\n",
    "        print('alpha', alphaNew, 'r2', scorepred)\n",
    "    return linData\n",
    "\n",
    "###############################################    \n",
    "# \n",
    "# LASSO\n",
    "#\n",
    "##############################################    \n",
    "\n",
    "def stdevRule(x, y, std):\n",
    "    \"\"\"move by one stdeviation to increase regularization.\"\"\"\n",
    "    yUp = np.min(y) + std[np.argmin(y)]\n",
    "    yFunc =  interp1d(x,y,'cubic',fill_value='extrapolate')\n",
    "#    plt.plot(x,y)\n",
    "#    plt.errorbar(x,y, yerr=std)\n",
    "#    plt.show()\n",
    "#    y0, y1 = np.min(y), y[-1]\n",
    "#    x0, x1 = x[np.argmin(y)], x[-1]\n",
    "#    m = (y1-y0)/(x1-x0)\n",
    "#    c = y0-m*x0\n",
    "#    xalpha = (c-yUp)/m\n",
    "    xalpha = x[np.argmin(y)]*1.5\n",
    "    xUpper = newton(lambda x: np.abs(yFunc(x) - yUp), xalpha)\n",
    "#    print xUpper, xalpha\n",
    "#    plt.plot(np.abs(yFunc(x) - yUp))\n",
    "#    \n",
    "#    plt.show()\n",
    "    return xUpper\n",
    "def balancedFolds(y, nSets=5,  splitMethod = 'unique'):\n",
    "    \"\"\"create balanced train/validate splitsby leave one out.\"\"\"\n",
    "    splits, _ = splitIntoSets(y, nBins=5, nSets=nSets, splitMethod=splitMethod, verbose=0)\n",
    "    folds = []\n",
    "    for i in range(len(splits)):\n",
    "        folds.append([splits[i], np.concatenate(splits[np.arange(len(splits))!=i] )])\n",
    "    return folds\n",
    "    \n",
    "def runLasso(data, pars, splits, plot = False, behaviors = ['AngleVelocity', 'Eigenworm3'], lag = None):\n",
    "    \"\"\"run LASSO to fit behavior and neural activity with a linear model.\"\"\"\n",
    "    linData = {}\n",
    "    for label in behaviors:\n",
    "        Y = np.reshape(np.copy(data['Behavior'][label]),(-1, 1))\n",
    "        trainingsInd, testInd = splits[label]['Train'], splits[label]['Test']\n",
    "    \n",
    "        if pars['useRank']:\n",
    "            X = data['Neurons']['rankActivity'].T\n",
    "        elif pars['useClust']:\n",
    "            clustres = runHierarchicalClustering(data, pars)\n",
    "            X = clustres['Activity'].T\n",
    "        else:\n",
    "            X = np.copy(data['Neurons']['Activity'].T) # transpose to conform to nsamples*nfeatures\n",
    "        # implement time lagging -- forward and reverse\n",
    "        \n",
    "        if lag is not None:\n",
    "            # move neural activity by lag (lag has units of volume)\n",
    "            # positive lag = behavior trails after neural activity, uses values from the past\n",
    "            # negative lag = behavior precedes neural activity\n",
    "            X = np.roll(X, shift = lag, axis = 0)\n",
    "        # prep data by scalig\n",
    "        # fit scale model\n",
    "        scale = False\n",
    "        if scale:\n",
    "            scalerX = preprocessing.StandardScaler().fit(X[trainingsInd])  \n",
    "            scalerY = preprocessing.StandardScaler().fit(np.array(list(Y))[trainingsInd])  \n",
    "            #scale data\n",
    "            X = scalerX.transform(X)\n",
    "            Y = scalerY.transform(Y)\n",
    "        Xtrain, Xtest = X[trainingsInd],X[testInd]\n",
    "        Ytrain, Ytest = np.array(list(Y))[trainingsInd],np.array(list(Y))[testInd]\n",
    "        # fit lasso and validate\n",
    "        #a = np.logspace(-2,2,100)\n",
    "        #cv = 10\n",
    "        # unbalanced sets\n",
    "#        fold = KFold(cv, shuffle=True) \n",
    "#        # balanced sets\n",
    "        if label =='Eigenworm3':\n",
    "            a = np.logspace(-3,-1,100)\n",
    "            nfold = 5#int(len(X)/250)\n",
    "        else:\n",
    "            a = np.logspace(-3,0,100)\n",
    "            nfold =5#%int(len(X)/500)\n",
    "        \n",
    "        #if label =='Eigenworm3':\n",
    "        #    nfold = balancedFolds(np.array(list(Y))[trainingsInd], nSets=cv)\n",
    "##        else:\n",
    "#        fold = balancedFolds(np.array(list(Y))[trainingsInd], nSets=cv)\n",
    "        #fold = 5\n",
    "        fold = TimeSeriesSplit(n_splits=nfold, max_train_size=None)\n",
    "        reg = linear_model.LassoCV(cv=fold,  verbose=0, \\\n",
    "         max_iter=10000, tol=1e-4)#, alphas = a)#, normalize=False)\n",
    "        \n",
    "        reg.fit(Xtrain, Ytrain)\n",
    "        alphas = reg.alphas_\n",
    "        ymean = np.mean(reg.mse_path_, axis =1)\n",
    "        yerr = np.std(reg.mse_path_, axis =1)/np.sqrt(nfold)\n",
    "        alphaNew = alphas[np.argmin(ymean)]\n",
    "#        # calculate standard deviation rule\n",
    "#        alphaNew = stdevRule(x = alphas, y= ymean, std= yerr)\n",
    "#        reg = linear_model.Lasso(alpha=alphaNew)\n",
    "#        reg.fit(X[trainingsInd], np.array(list(Y))[trainingsInd])\n",
    "        \n",
    "        if plot:\n",
    "            plt.subplot(221)\n",
    "            plt.title('Trainingsset')\n",
    "            plt.plot(Ytrain, 'r')\n",
    "            plt.plot(reg.predict(Xtrain), 'k', alpha=0.7)\n",
    "            plt.subplot(222)\n",
    "            plt.title('Testset')\n",
    "            plt.plot(np.array(list(Y))[testInd], 'r')\n",
    "            plt.plot(reg.predict(Xtest), 'k', alpha=0.7)\n",
    "            ax1 = plt.subplot(223)\n",
    "            plt.title('Non-zero weighths: {}'.format(len(reg.coef_[reg.coef_!=0])))\n",
    "            ax1.scatter(Ytest, reg.predict(Xtest), alpha=0.7, s=0.2)\n",
    "#            hist, bins = np.histogram(reg.coef_, bins = 30, density = True)\n",
    "#            ax1.fill_between(bins[:-1],np.zeros(len(hist)), hist, step='post', color='r')\n",
    "#            ax1.set_xlabel(r'weights')\n",
    "#            ax1.set_ylabel('PDF(weights)')\n",
    "            plt.subplot(224)\n",
    "            \n",
    "            plt.plot(reg.alphas_, reg.mse_path_, 'k', alpha = 0.3)\n",
    "            plt.plot(alphas, ymean, 'k')\n",
    "            plt.errorbar(alphas, ymean, yerr=yerr, capsize=1)\n",
    "            plt.axvline(alphas[np.argmin(ymean)],label='minimal error')\n",
    "            plt.axvline(alphaNew,label='stdev rule')\n",
    "            plt.xscale('log')\n",
    "            #plt.fill_between(reg.alphas_,y1=ymean-yerr, y2= ymean+yerr, alpha=0.5)\n",
    "            #plt.errorbar(,color= 'k')\n",
    "            plt.tight_layout()            \n",
    "            plt.show(block=True)\n",
    "        weights = reg.coef_\n",
    "        # score model\n",
    "        if len(weights)>0:\n",
    "            # normalize testset with scaler values\n",
    "            scorepred = reg.score(Xtest, Ytest)#, sample_weight=np.power(np.array(list(Y))[testInd], 2))\n",
    "            score = reg.score(Xtrain, Ytrain)\n",
    "        else:\n",
    "            scorepred = np.nan\n",
    "            score = reg.score(Xtrain, Ytrain)\n",
    "        linData[label] = {}\n",
    "        linData[label]['weights'] =  weights\n",
    "        linData[label]['intercepts'] = reg.intercept_\n",
    "        linData[label]['alpha'] = alphaNew#reg.alpha_\n",
    "        linData[label]['score'] = score\n",
    "        linData[label]['scorepredicted'] = scorepred\n",
    "        linData[label]['noNeurons'] = len(reg.coef_[np.abs(reg.coef_)>0])\n",
    "        if scale:\n",
    "            linData[label]['output'] = scalerY.inverse_transform(reg.predict(X)) # full output training and test\n",
    "        else:\n",
    "            linData[label]['output'] = reg.predict(X)\n",
    "        print('alpha', alphaNew, 'r2', scorepred)\n",
    "    return linData\n",
    "    \n",
    "###############################################    \n",
    "# \n",
    "# ElasticNet\n",
    "#\n",
    "##############################################    \n",
    "\n",
    "def runElasticNet(data, pars, splits, plot = False, scramble = False, behaviors = ['AngleVelocity', 'Eigenworm3'], lag = None):\n",
    "    \"\"\"run EN to fit behavior and neural activity with a linear model.\"\"\"\n",
    "    linData = {}\n",
    "    for label in behaviors:\n",
    "        Y = np.copy(data['Behavior'][label])\n",
    "        Y = np.reshape(Y, (-1,1))\n",
    "        #Y = preprocessing.scale(Y)\n",
    "        if pars['useRank']:\n",
    "            X = np.copy(data['Neurons']['rankActivity'].T)\n",
    "        if pars['useRaw']:\n",
    "            X = np.copy(data['Neurons']['RawActivity'].T)\n",
    "            X -= np.mean(X, axis = 0)\n",
    "        elif pars['useClust']:\n",
    "            clustres = runHierarchicalClustering(data, pars)\n",
    "            X = clustres['Activity'].T\n",
    "        elif pars['useDeconv']:\n",
    "            X = data['Neurons']['deconvolvedActivity'].T\n",
    "        else:\n",
    "            X = np.copy(data['Neurons']['Activity']).T # transpose to conform to nsamples*nfeatures\n",
    "        if scramble:\n",
    "            # similar to GFP control: scamble timeseries\n",
    "            np.random.shuffle(Y)\n",
    "        trainingsInd, testInd = splits[label]['Train'], splits[label]['Test']\n",
    "        # implement time lagging -- forward and reverse\n",
    "        if lag is not None:\n",
    "            # move neural activity by lag (lag has units of volume)\n",
    "            # positive lag = behavior lags after neural activity, uses values from the past\n",
    "            X = np.roll(X, shift = lag, axis = 0)\n",
    "        # fit scale model\n",
    "        scale = 1\n",
    "        if scale:\n",
    "            scalerX = preprocessing.StandardScaler().fit(X[trainingsInd])  \n",
    "            scalerY = preprocessing.StandardScaler().fit(np.array(list(Y))[trainingsInd])  \n",
    "            #scale data\n",
    "            X = scalerX.transform(X)\n",
    "            Y = scalerY.transform(Y)\n",
    "        Xtrain, Xtest = X[trainingsInd],X[testInd]\n",
    "        Ytrain, Ytest = np.array(list(Y))[trainingsInd],np.array(list(Y))[testInd]\n",
    "        # fit elasticNet and validate\n",
    "        \n",
    "        if label =='Eigenworm3':\n",
    "            l1_ratio = [0.99]\n",
    "            #l1_ratio = [0.95]\n",
    "            #fold =10\n",
    "            #fold = balancedFolds(np.array(list(Y))[trainingsInd], nSets=cv)\n",
    "            a = np.logspace(-2,-0.5,200)\n",
    "            nfold = 5\n",
    "            tol = 1e-10\n",
    "        else:\n",
    "            #l1_ratio = [0.5, 0.7, 0.8, .9, .95,.99, 1]\n",
    "            l1_ratio = [0.99]\n",
    "            #fold = balancedFolds(np.array(list(Y))[trainingsInd], nSets=cv)\n",
    "            a = np.logspace(-4,-1,200)\n",
    "            nfold = 5\n",
    "            tol = 1e-10\n",
    "            \n",
    "        #cv = 15\n",
    "        #a = np.logspace(-3,-1,100)\n",
    "       # fold = 5\n",
    "        fold = TimeSeriesSplit(n_splits=nfold, max_train_size=None)\n",
    "        reg = linear_model.ElasticNetCV(l1_ratio, cv=fold, verbose=0, selection='random', tol=tol, alphas=a)\n",
    "        #        \n",
    "        reg.fit(Xtrain, Ytrain)\n",
    "\n",
    "        scorepred = reg.score(Xtest, Ytest)\n",
    "        score = reg.score(Xtrain, Ytrain)\n",
    "        \n",
    "        #linData[label] = [reg.coef_, reg.intercept_, reg.alpha_, score, scorepred]\n",
    "        linData[label] = {}\n",
    "        linData[label]['weights'] = reg.coef_\n",
    "        linData[label]['intercepts'] = reg.intercept_\n",
    "        linData[label]['alpha'] = reg.alpha_\n",
    "        linData[label]['l1_ratio'] = reg.l1_ratio_\n",
    "        linData[label]['score'] = score\n",
    "        linData[label]['scorepredicted'] = scorepred\n",
    "        linData[label]['noNeurons'] = len(reg.coef_[np.abs(reg.coef_)>0])\n",
    "        print('R2', scorepred, 'N', len(reg.coef_[np.abs(reg.coef_)>0]))\n",
    "        if scale:\n",
    "            linData[label]['output'] = scalerY.inverse_transform(reg.predict(X)) # full output training and test\n",
    "        else:\n",
    "            linData[label]['output'] = reg.predict(X)\n",
    "        if plot:\n",
    "            print('alpha', reg.alpha_, 'l1_ratio', reg.l1_ratio_, 'r2', scorepred)\n",
    "            \n",
    "            plt.subplot(221)\n",
    "            plt.title('Trainingsset')\n",
    "            plt.plot(Ytrain, 'r')\n",
    "            plt.plot(reg.predict(Xtrain), 'k', alpha=0.7)\n",
    "            plt.subplot(222)\n",
    "            plt.title('Testset')\n",
    "            plt.plot(np.array(list(Y))[testInd], 'r')\n",
    "            plt.plot(reg.predict(Xtest), 'k', alpha=0.7)\n",
    "            ax1 = plt.subplot(223)\n",
    "            plt.title('Non-zero weighths: {}'.format(len(reg.coef_[reg.coef_!=0])))\n",
    "            ax1.scatter(Ytest, reg.predict(Xtest), alpha=0.7, s=0.2)\n",
    "            #hist, bins = np.histogram(reg.coef_, bins = 30, density = True)\n",
    "            #ax1.fill_between(bins[:-1],np.zeros(len(hist)), hist, step='post', color='r')\n",
    "            #ax1.set_xlabel(r'weights')\n",
    "            #ax1.set_ylabel('PDF(weights)')\n",
    "            plt.subplot(224)\n",
    "            # use if only one l1_ratio\n",
    "            if len(l1_ratio)==1:\n",
    "                plt.plot(reg.alphas_, reg.mse_path_, 'k', alpha = 0.1)\n",
    "                plt.plot(reg.alphas_, np.mean(reg.mse_path_, axis =1))\n",
    "            else:\n",
    "                if len(reg.alphas_.shape)>1:\n",
    "                    for l1index, l1 in enumerate(l1_ratio):\n",
    "                        plt.plot(reg.alphas_[lindex], reg.mse_path_[l1index], 'k', alpha = 0.1)\n",
    "                        plt.plot(reg.alphas_[lindex], np.mean(reg.mse_path_[l1index], axis =1))\n",
    "                else:\n",
    "                    for l1index, l1 in enumerate(l1_ratio):\n",
    "                        plt.plot(reg.alphas_, reg.mse_path_[l1index], 'k', alpha = 0.1)\n",
    "                        plt.plot(reg.alphas_, np.mean(reg.mse_path_[l1index], axis =1))\n",
    "            plt.tight_layout()\n",
    "            plt.show(block=True)\n",
    "    return linData\n",
    "\n",
    "###############################################    \n",
    "# \n",
    "# run LASSO with multiple time lags and collate data\n",
    "#\n",
    "##############################################\n",
    "    \n",
    "def timelagRegression(data, pars, splits, plot = False, behaviors = ['AngleVelocity', 'Eigenworm3'], flag = 'LASSO', lags = np.arange(-18,19, 3)):\n",
    "    \"\"\"runs LASSO in the same train/test split for multiple time lags and computes the standard erro, parameters etc.\"\"\"\n",
    "    # store results\n",
    "    res = []\n",
    "    for lag in lags:\n",
    "        if flag =='LASSO':\n",
    "            results = runLasso(data, pars, splits, plot = False, behaviors = behaviors, lag = lag)\n",
    "        else:\n",
    "            results = runElasticNet(data, pars, splits, plot = False, behaviors = behaviors, lag = lag)\n",
    "        res.append(results)\n",
    "    # pull out the results\n",
    "    pcares = {}\n",
    "    for rindex, results in enumerate(res):\n",
    "        for lindex, label in enumerate(behaviors):\n",
    "            if rindex == 0:\n",
    "                pcares[label] = {}\n",
    "                pcares[label]['lags'] = lags\n",
    "            for key in results[label].keys():\n",
    "                if rindex==0:\n",
    "                    pcares[label][key] = []\n",
    "                pcares[label][key].append(results[label][key])\n",
    "                \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.subplot(221)\n",
    "    for label in behaviors:\n",
    "        plt.plot(pcares[label]['lags'], pcares[label]['scorepredicted'], label=label)\n",
    "    plt.subplot(222)\n",
    "    for label in behaviors:\n",
    "        plt.plot(pcares[label]['lags'], pcares[label]['noNeurons'], label=label)\n",
    "    plt.subplot(223)\n",
    "    plt.imshow(np.array(pcares[behaviors[0]]['weights']), label=label, aspect='auto')\n",
    "    plt.subplot(224)\n",
    "    plt.imshow(np.array(pcares[behaviors[1]]['weights']), label=label, aspect='auto')\n",
    "    plt.show()\n",
    "    return pcares\n",
    "    \n",
    "###############################################    \n",
    "# \n",
    "# run LASSO/EN with multiple train-test splits to obtain unbiased estimate of test error\n",
    "# also check how robust model fitting is \n",
    "#\n",
    "##############################################\n",
    "    \n",
    "def NestedRegression(data, pars, splits, plot = False, behaviors = ['AngleVelocity', 'Eigenworm3'], flag = 'LASSO'):\n",
    "    \"\"\"runs LASSO in the same train/test split for multiple time lags and computes the standard erro, parameters etc.\"\"\"\n",
    "    # store results\n",
    "    res = []\n",
    "    # since this is a time series, make sure test is after all training data\n",
    "    timeLen = data['Neurons']['Activity'].shape[1]\n",
    "    dur = pars['testVolumes'] # 2 min of data for test sets\n",
    "    # make successive train-test splits with increasing time ('day-forward chaining')\n",
    "    maxSplits = int((timeLen-dur)/dur)\n",
    "\n",
    "    splitOuterLoop = [(np.arange(0, (i+1)*dur), np.arange((i+1)*dur, (i+2)*dur)) for i in range(maxSplits)]\n",
    "    for repeats in splitOuterLoop:\n",
    "        plt.plot(repeats[0])\n",
    "        plt.plot(repeats[1])\n",
    "        plt.show()\n",
    "    splits = {}    \n",
    "    \n",
    "    for repeats in splitOuterLoop:\n",
    "        for label in behaviors:\n",
    "            splits[label] = {}\n",
    "            splits[label]['Train'] , splits[label]['Test'] = repeats\n",
    "        \n",
    "        if flag =='LASSO':\n",
    "            results = runLasso(data, pars, splits, plot = True, behaviors = behaviors, lag = None)\n",
    "        else:\n",
    "            results = runElasticNet(data, pars, splits, plot = False, behaviors = behaviors, lag = None)\n",
    "        res.append(results)\n",
    "    # pull out the results\n",
    "    pcares = {}\n",
    "    for rindex, results in enumerate(res):\n",
    "        for lindex, label in enumerate(behaviors):\n",
    "            if rindex == 0:\n",
    "                pcares[label] = {}\n",
    "                \n",
    "            for key in results[label].keys():\n",
    "                if rindex==0:\n",
    "                    pcares[label][key] = []\n",
    "                pcares[label][key].append(results[label][key])\n",
    "                \n",
    "    plt.figure()\n",
    "    plt.subplot(221)\n",
    "    for label in behaviors:\n",
    "        plt.plot( pcares[label]['scorepredicted'], label=label)\n",
    "    plt.subplot(222)\n",
    "    for label in behaviors:\n",
    "        plt.plot( pcares[label]['noNeurons'], label=label)\n",
    "    plt.subplot(223)\n",
    "    plt.imshow(np.array(pcares[behaviors[0]]['weights']), label=label, aspect='auto')\n",
    "    plt.subplot(224)\n",
    "    plt.imshow(np.array(pcares[behaviors[1]]['weights']), label=label, aspect='auto')\n",
    "    plt.show()\n",
    "    return pcares\n",
    "    \n",
    "###############################################    \n",
    "# \n",
    "# calculate non-linearity from linear model output\n",
    "#\n",
    "##############################################\n",
    "def fitNonlinearity(data, results, splits, pars, fitmethod = 'LASSO', behaviors = ['AngleVelocity', 'Eigenworm3']):\n",
    "    \"\"\"fit a nonlinearity to a dataset that was fit with a linear model first.\"\"\"\n",
    "    for label in behaviors:\n",
    "        # linear model output\n",
    "        X = results[fitmethod][label]['output']\n",
    "        # true values\n",
    "        Y =  data['Behavior'][label]\n",
    "        # \n",
    "        plt.hexbin(X,Y)\n",
    "        plt.show()\n",
    "###############################################    \n",
    "# \n",
    "# Show how prediction improves with more neurons\n",
    "#\n",
    "##############################################  \n",
    "def scoreModelProgression(data, results, splits, pars, fitmethod = 'LASSO', behaviors = ['AngleVelocity', 'Eigenworm3']):\n",
    "    \"\"\"show how more neurons improve predictive abilities.\"\"\"\n",
    "    linData = {}\n",
    "    for label in behaviors:\n",
    "        # get the weights from previously fit data and sort by absolute amplitude\n",
    "        weights = results[fitmethod][label]['weights']\n",
    "        weightsInd = np.argsort(np.abs(weights))[::-1]\n",
    "        \n",
    "        # sort neurons by weight\n",
    "        Y = data['Behavior'][label]\n",
    "        if pars['useRank']:\n",
    "            X = data['Neurons']['rankActivity'].T\n",
    "        else:\n",
    "            X = np.copy(data['Neurons']['Activity']).T # transpose to conform to nsamples*nfeatures\n",
    "        trainingsInd, testInd = splits[label]['Train'], splits[label]['Test']\n",
    "        # individual predictive scores\n",
    "        indScore = []\n",
    "        sumScore = []\n",
    "        mse = []\n",
    "        print(\"___________________________\")\n",
    "        print(fitmethod, 'params:', results[fitmethod][label]['alpha'])\n",
    "        print(fitmethod, 'R2:', results[fitmethod][label]['scorepredicted'], results[fitmethod][label]['score'])\n",
    "\n",
    "        for count, wInd in enumerate(weightsInd):\n",
    "            if np.abs(weights[wInd]) >0:\n",
    "                # fit one neuron\n",
    "                if fitmethod == 'LASSO':\n",
    "                    reg = linear_model.Lasso(alpha = results[fitmethod][label]['alpha'])\n",
    "                elif fitmethod == 'ElasticNet':\n",
    "                    \n",
    "                    reg = linear_model.ElasticNet(alpha = results[fitmethod][label]['alpha'],\n",
    "                                                  l1_ratio = results[fitmethod][label]['l1_ratio'], tol=1e-5, selection='random')\n",
    "                #reg = linear_model.LinearRegression()\n",
    "                \n",
    "                xTmp = np.reshape(X[:,weightsInd[:count+1]], (-1,count+1))\n",
    "                reg.fit(xTmp[trainingsInd], np.array(list(Y))[trainingsInd])\n",
    "                    \n",
    "                \n",
    "                sumScore.append(reg.score(xTmp[testInd], np.array(list(Y))[testInd]))\n",
    "                mse.append(np.sum((reg.predict(xTmp[testInd])-np.array(list(Y))[testInd])**2))\n",
    "                \n",
    "                xTmp = np.reshape(X[:,wInd], (-1,1))\n",
    "                reg.fit(xTmp[trainingsInd], np.array(list(Y))[trainingsInd])\n",
    "                indScore.append(reg.score(xTmp[testInd], np.array(list(Y))[testInd]))\n",
    "                \n",
    "        linData[label] = {}\n",
    "        linData[label]['cumulativeScore'] = sumScore\n",
    "        linData[label]['individualScore'] = indScore\n",
    "        linData[label]['MSE'] = mse\n",
    "    return linData\n",
    "    \n",
    "###############################################    \n",
    "# \n",
    "# reorganize linear models to fit PCA plot style\n",
    "#\n",
    "##############################################\n",
    "def reorganizeLinModel(data, results, splits, pars, fitmethod = 'LASSO', behaviors = ['AngleVelocity', 'Eigenworm3']):\n",
    "    \"\"\"takes a model fit and calculates basis vectors etc.\"\"\"\n",
    "    # modify data to be like scikit likes it\n",
    "    if pars['useRank']:\n",
    "        X = data['Neurons']['rankActivity'].T\n",
    "    elif pars['useClust']:\n",
    "        clustres = runHierarchicalClustering(data, pars)\n",
    "        X = clustres['Activity']\n",
    "    else:\n",
    "        X = np.copy(data['Neurons']['Activity']).T # transpose to conform to nsamples*nfeatures\n",
    "    # get weights\n",
    "    pcs = np.vstack([results[fitmethod][label]['weights'] for label in behaviors])\n",
    "    indices = np.argsort(pcs[0])\n",
    "    \n",
    "    comp = np.zeros((len(behaviors), len(X)))\n",
    "    # show temporal components created by fits\n",
    "    for wi, weights in enumerate(pcs):\n",
    "        comp[wi] = np.dot(X, weights)\n",
    "    # recreate neural data from weights\n",
    "    neurPred = np.zeros(X.shape)\n",
    "    \n",
    "    # calculate predicted neural dynamics  TODO\n",
    "    score = np.ones(len(behaviors))\n",
    "    #score = explained_variance_score()\n",
    "    \n",
    "    pcares = {}\n",
    "    pcares['nComp'] =  len(behaviors)\n",
    "    pcares['expVariance'] =  score\n",
    "    pcares['neuronWeights'] =  pcs.T\n",
    "    pcares['neuronOrderPCA'] =  indices\n",
    "    pcares['pcaComponents'] =  comp\n",
    "    return pcares\n",
    "\n",
    "\n",
    "###############################################    \n",
    "# \n",
    "# predict neural dynamics from behavior\n",
    "#\n",
    "##############################################\n",
    "def predictNeuralDynamicsfromBehavior(data,  splits, pars):\n",
    "    \"\"\"use linear model to predict the neural data from behavior and estimate what worm is thinking about.\"\"\"\n",
    "    label = 'AngleVelocity'\n",
    "    train, test = splits[label]['Train'], splits[label]['Test']\n",
    "    # create dimensionality-reduced behavior - pca with 10 eigenworms\n",
    "    cl = data['CL']\n",
    "    eigenworms = dh.loadEigenBasis(filename = 'utility/Eigenworms.dat', nComp=4, new=True)\n",
    "    pcsNew, meanAngle, lengths, refPoint = dh.calculateEigenwormsFromCL(cl, eigenworms)\n",
    "    behavior = pcsNew.T\n",
    "    # nevermind, use the behaviors we use for lasso instead\n",
    "    behavior = np.vstack([data['Behavior']['AngleVelocity'], behavior.T]).T\n",
    "    # scale behavior to same \n",
    "    behavior = preprocessing.scale(behavior)\n",
    "    blabels = np.array(['Wave velocity', 'Eigenworm 3', 'Eigenworm 2', 'Eigenworm 1', 'Eigenworm 4'])\n",
    "\n",
    "    #pcsNew are new eigenworms directly from the centerlines. \n",
    "    # also reduce dimensionality of the neural dynamics.\n",
    "    nComp = 10#pars['nCompPCA']\n",
    "    pca = PCA(n_components = nComp)\n",
    "    Neuro = np.copy(data['Neurons']['Activity']).T\n",
    "    pcs = pca.fit_transform(Neuro)\n",
    "    #comp = pca.components_.T\n",
    "    #now we use a linear model to train and test our predictions\n",
    "    #here we will simply use a 50/50 split for now\n",
    "    half = int((len(Neuro))/2.)\n",
    "    tmp = np.arange(2*half)\n",
    "    test = tmp[int(half/2.):int(3*half/2.)]\n",
    "    train = np.setdiff1d(tmp, test)\n",
    "    \n",
    "    #train = np.arange(half)\n",
    "    #test = np.arange(half,2*half)\n",
    "    # lets build a linear model\n",
    "    lin = linear_model.LinearRegression(normalize=False)\n",
    "    lin.fit(behavior[train], pcs[train])\n",
    "    \n",
    "    # some fit diagnostics\n",
    "#    for i in range(nComp):\n",
    "    print('Train R2: ', lin.score(behavior[train],pcs[train]))\n",
    "    print('Test R2: ', lin.score(behavior[test],pcs[test]))\n",
    "    # recreate the PCA components of the neural map from these predictions\n",
    "    predN = lin.predict(behavior)\n",
    "    print(predN.shape, 'predicted neurons in pca space', behavior.shape)\n",
    "    \n",
    "    indices = np.argsort(explained_variance_score(pcs[test],predN[test],multioutput ='raw_values'))[::-1]\n",
    "    r2 = [explained_variance_score(pcs[test,i], predN[test,i]) for i in indices]    \n",
    "    # weightorder\n",
    "    weightorder = np.arange(lin.coef_.shape[1])[np.argsort(np.abs(lin.coef_[indices[0]]))][::-1]\n",
    "    # reverse the PCA\n",
    "    newHM = pca.inverse_transform(predN).T\n",
    "    # order the weights for each behavior\n",
    "    #orderedWeights = lin.coef_[:,weightorder]\n",
    "    # variance explained with each PC prediction\n",
    "    expScore = []\n",
    "    for i in range(len(weightorder)):\n",
    "        tmpBeh = behavior[:,weightorder]\n",
    "        tmpBeh[:,i+1:] = 0\n",
    "        predictedNeurons = lin.predict(tmpBeh)\n",
    "        tmpHM = pca.inverse_transform(predictedNeurons)\n",
    "        # compare to full neural data\n",
    "        #expScore.append(explained_variance_score(data['Neurons']['Activity'][test], tmpHM[test]))\n",
    "        # compare to nComp recornstructed data\n",
    "        print(pca.inverse_transform(pcs).shape, tmpHM.shape)\n",
    "        expScore.append(explained_variance_score(pca.inverse_transform(pcs)[test], tmpHM[test]))\n",
    "    # store results\n",
    "    pcares = {}\n",
    "    pcares['nComp'] =  nComp\n",
    "    pcares['lowDimNeuro'] = pca.inverse_transform(pcs).T# smaller dimension neural data\n",
    "    pcares['behavior'] = behavior # stacked behavioral vectors\n",
    "    pcares['expVariance'] =  expScore\n",
    "    pcares['behaviorWeights'] =  lin.coef_# actual fitted weights for each behavior\n",
    "    pcares['behaviorOrder'] =  weightorder # indices of behaviors if ordered by fit weight\n",
    "    pcares['predictedNeuralPCS'] =  predN\n",
    "    pcares['NeuralPCS'] =  pcs\n",
    "    pcares['behaviorLabels'] = blabels    \n",
    "    pcares['predictedNeuralDynamics'] = newHM\n",
    "    pcares['PCA_indices'] = indices\n",
    "    pcares['R2_test'] = r2# r2 for predicting neural PCS testset only()\n",
    "\n",
    "    return pcares\n",
    "\n",
    "\n",
    "###############################################    \n",
    "# \n",
    "# predict behavior from 3 PCA axes\n",
    "#\n",
    "##############################################\n",
    "def predictBehaviorFromPCA(data,  splits, pars, behaviors):\n",
    "    linData = {}\n",
    "    for label in behaviors:\n",
    "        train, test = splits[label]['Train'], splits[label]['Test']\n",
    "        # create dimensionality-reduced behavior - pca with 10 eigenworms\n",
    "        # nevermind, use the behaviors we use for lasso instead\n",
    "        behavior = data['Behavior'][label]\n",
    "        # scale behavior to same \n",
    "        behavior = preprocessing.scale(behavior)\n",
    "       \n",
    "        # also reduce dimensionality of the neural dynamics.\n",
    "        nComp = 3#pars['nCompPCA']\n",
    "        pca = PCA(n_components = nComp)\n",
    "        Neuro = np.copy(data['Neurons']['Activity']).T\n",
    "        pcs = pca.fit_transform(Neuro)\n",
    "        #now we use a linear model to train and test our predictions\n",
    "        # lets build a linear model\n",
    "        lin = linear_model.LinearRegression(normalize=False)\n",
    "        lin.fit(pcs[train], behavior[train])\n",
    "        \n",
    "        score = lin.score(pcs[train],behavior[train])\n",
    "        scorepred = lin.score(pcs[test], behavior[test])\n",
    "        print('PCA prediction results:')\n",
    "        print('Train R2: ',score )\n",
    "        print('Test R2: ', scorepred)\n",
    "        linData[label] = {}\n",
    "        linData[label]['weights'] =  lin.coef_\n",
    "        linData[label]['intercepts'] = lin.intercept_\n",
    "        \n",
    "        linData[label]['score'] = score\n",
    "        linData[label]['scorepredicted'] = scorepred\n",
    "        \n",
    "        linData[label]['output'] = lin.predict(pcs)\n",
    "        \n",
    "#        # check how well it performs with more PCs\n",
    "#        r2_more = []\n",
    "#        for nC in range(1, Neuro.shape[1], 1):\n",
    "#            pca = PCA(n_components = nC)\n",
    "#            pcs = pca.fit_transform(Neuro)\n",
    "#            lin.fit(pcs[train], behavior[train])\n",
    "#            r2_more.append(lin.score(pcs[test], behavior[test]))\n",
    "#            \n",
    "#        linData[label]['r2PCS'] = r2_more\n",
    "#        linData[label]['r2PCSx'] = range(1, Neuro.shape[1], 1)\n",
    "        \n",
    "    return linData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
